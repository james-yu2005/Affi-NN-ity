{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "b2db1471",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac4f713-e342-4f31-ad8b-71088cd20d85"
      },
      "source": [
        "!pip install -q deepchem torch_geometric rdkit biopython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.4/552.4 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import deepchem as dc\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from torch.nn import Linear, ReLU, Sequential, BatchNorm1d, Dropout\n",
        "from torch_geometric.nn import GINConv, global_add_pool, HeteroConv, SAGEConv, global_mean_pool, GATv2Conv, Linear\n",
        "from torch_geometric.data import Data, HeteroData, Dataset\n",
        "from torch.serialization import add_safe_globals\n",
        "from torch_geometric.data.storage import BaseStorage\n",
        "from scipy.spatial import distance_matrix\n",
        "from rdkit import Chem\n",
        "from Bio import PDB\n",
        "# from torch_geometric.utils import scatter\n",
        "add_safe_globals([BaseStorage, Data])"
      ],
      "metadata": {
        "id": "MuQOwapDBASD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0ef504-65d0-4c20-8ed1-2264711a41a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GraphDock Functions"
      ],
      "metadata": {
        "id": "qI6URjK1_-Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ligand_atoms_features(ligand_path):\n",
        "    \"\"\"Extract ligand atom features and 3D coordinates for GraphDock.\"\"\"\n",
        "    from rdkit import Chem\n",
        "    # Suppress RDKit warnings temporarily\n",
        "    from rdkit import RDLogger\n",
        "    RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "    if ligand_path.endswith('.pdb'):\n",
        "        mol = Chem.MolFromPDBFile(str(ligand_path), removeHs=False)\n",
        "    elif ligand_path.endswith('.mol2'):\n",
        "        mol = Chem.MolFromMol2File(str(ligand_path), removeHs=False)\n",
        "    else:\n",
        "        mol = Chem.MolFromMolFile(str(ligand_path), removeHs=False)\n",
        "\n",
        "    if mol is None:\n",
        "        return None, None, None\n",
        "\n",
        "    conf = mol.GetConformer()\n",
        "    coords = np.array([list(conf.GetAtomPosition(i)) for i in range(mol.GetNumAtoms())])\n",
        "\n",
        "    features = []\n",
        "    atom_types = []\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "        atom_num = atom.GetAtomicNum()\n",
        "        atom_types.append(atom_num)\n",
        "\n",
        "        atom_type_onehot = [0] * 11\n",
        "        type_map = {6: 0, 7: 1, 8: 2, 16: 3, 15: 4, 9: 5, 17: 6, 35: 7, 53: 8, 1: 9}\n",
        "        atom_type_onehot[type_map.get(atom_num, 10)] = 1\n",
        "\n",
        "        feat = atom_type_onehot + [\n",
        "            atom.GetDegree() / 6.0,\n",
        "            atom.GetFormalCharge(),\n",
        "            int(atom.GetHybridization()) / 6.0,\n",
        "            int(atom.GetIsAromatic()),\n",
        "            atom.GetTotalNumHs() / 4.0,\n",
        "            atom.GetMass() / 100.0,\n",
        "            int(atom.IsInRing()),\n",
        "        ]\n",
        "        features.append(feat)\n",
        "\n",
        "    return coords, np.array(features, dtype=np.float32), atom_types\n",
        "\n",
        "def get_protein_atoms_features(protein_path):\n",
        "    \"\"\"Extract protein pocket atom features and 3D coordinates for GraphDock with enhanced features.\"\"\"\n",
        "    from Bio import PDB\n",
        "    from Bio.PDB import DSSP\n",
        "\n",
        "    parser = PDB.PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"pocket\", protein_path)\n",
        "\n",
        "    # Try to compute DSSP for secondary structure\n",
        "    try:\n",
        "        model = structure[0]\n",
        "        dssp = DSSP(model, protein_path, dssp='mkdssp')\n",
        "        dssp_dict = dict(dssp)\n",
        "    except:\n",
        "        dssp_dict = None\n",
        "\n",
        "    coords = []\n",
        "    features = []\n",
        "    residue_types = []\n",
        "\n",
        "    # Enhanced amino acid properties: [hydrophobic, charge, polar, aromatic, size, pKa]\n",
        "    aa_properties = {\n",
        "        'ALA': [1, 0, 0, 0, 0.3, 0],\n",
        "        'ARG': [0, 1, 1, 0, 0.9, 12.5],\n",
        "        'ASN': [0, 0, 1, 0, 0.5, 0],\n",
        "        'ASP': [0, -1, 1, 0, 0.5, 3.9],\n",
        "        'CYS': [1, 0, 1, 0, 0.4, 8.3],\n",
        "        'GLN': [0, 0, 1, 0, 0.6, 0],\n",
        "        'GLU': [0, -1, 1, 0, 0.6, 4.2],\n",
        "        'GLY': [0, 0, 0, 0, 0.2, 0],\n",
        "        'HIS': [0, 0.5, 1, 1, 0.6, 6.0],\n",
        "        'ILE': [1, 0, 0, 0, 0.7, 0],\n",
        "        'LEU': [1, 0, 0, 0, 0.7, 0],\n",
        "        'LYS': [0, 1, 1, 0, 0.7, 10.5],\n",
        "        'MET': [1, 0, 0, 0, 0.7, 0],\n",
        "        'PHE': [1, 0, 0, 1, 0.8, 0],\n",
        "        'PRO': [0, 0, 0, 0, 0.5, 0],\n",
        "        'SER': [0, 0, 1, 0, 0.4, 0],\n",
        "        'THR': [0, 0, 1, 0, 0.5, 0],\n",
        "        'TRP': [1, 0, 0, 1, 1.0, 0],\n",
        "        'TYR': [0, 0, 1, 1, 0.9, 10.1],\n",
        "        'VAL': [1, 0, 0, 0, 0.6, 0],\n",
        "    }\n",
        "\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                res_name = residue.get_resname()\n",
        "\n",
        "                # Get secondary structure for this residue\n",
        "                if dssp_dict:\n",
        "                    try:\n",
        "                        key = (chain.id, residue.id)\n",
        "                        ss = dssp_dict[key][2]\n",
        "                        acc = dssp_dict[key][3]  # Solvent accessibility\n",
        "\n",
        "                        # Secondary structure encoding\n",
        "                        if ss in ['H', 'G', 'I']:  # Helices\n",
        "                            ss_feat = [1, 0, 0]\n",
        "                        elif ss in ['E', 'B']:  # Sheets\n",
        "                            ss_feat = [0, 1, 0]\n",
        "                        else:  # Coils and others\n",
        "                            ss_feat = [0, 0, 1]\n",
        "\n",
        "                        # Normalize accessibility (typical max is ~200)\n",
        "                        acc_feat = min(acc / 200.0, 1.0)\n",
        "                    except:\n",
        "                        ss_feat = [0, 0, 1]  # Default to coil\n",
        "                        acc_feat = 0.5  # Unknown accessibility\n",
        "                else:\n",
        "                    ss_feat = [0, 0, 1]\n",
        "                    acc_feat = 0.5\n",
        "\n",
        "                for atom in residue:\n",
        "                    coords.append(atom.get_coord())\n",
        "                    element = atom.element.strip()\n",
        "\n",
        "                    # Atom type one-hot\n",
        "                    atom_type_onehot = [0] * 5\n",
        "                    type_map = {'C': 0, 'N': 1, 'O': 2, 'S': 3}\n",
        "                    atom_type_onehot[type_map.get(element, 4)] = 1\n",
        "\n",
        "                    # Get enhanced residue properties\n",
        "                    res_props = aa_properties.get(res_name, [0, 0, 0, 0, 0.5, 0])\n",
        "\n",
        "                    # Normalize pKa\n",
        "                    res_props_normalized = res_props[:5] + [res_props[5] / 14.0]  # pH scale 0-14\n",
        "\n",
        "                    is_backbone = int(atom.name in ['N', 'CA', 'C', 'O'])\n",
        "\n",
        "                    # Combine all features:\n",
        "                    # 5 (atom type) + 6 (residue props) + 1 (backbone) + 3 (secondary structure) + 1 (accessibility)\n",
        "                    feat = atom_type_onehot + res_props_normalized + [is_backbone] + ss_feat + [acc_feat]\n",
        "                    features.append(feat)\n",
        "                    residue_types.append(res_name)\n",
        "\n",
        "    return np.array(coords), np.array(features, dtype=np.float32), residue_types\n",
        "\n",
        "\n",
        "def compute_edge_features_graphdock(coord_i, coord_j, dist):\n",
        "    \"\"\"Compute edge features for GraphDock.\"\"\"\n",
        "    distance_feat = [dist]\n",
        "    rbf_centers = np.linspace(0, 10, 10)\n",
        "    rbf_gamma = 0.5\n",
        "    rbf_feats = np.exp(-rbf_gamma * (dist - rbf_centers) ** 2).tolist()\n",
        "\n",
        "    if dist > 0:\n",
        "        direction = (coord_j - coord_i) / dist\n",
        "        direction_feats = direction.tolist()\n",
        "    else:\n",
        "        direction_feats = [0.0, 0.0, 0.0]\n",
        "\n",
        "    return distance_feat + rbf_feats + direction_feats\n",
        "\n",
        "\n",
        "def create_graphdock_hetero_graph(ligand_path, protein_path, ligand_cutoff=5.0,\n",
        "                                   protein_cutoff=6.0, interaction_cutoff=5.0):\n",
        "    \"\"\"Create a heterogeneous graph for GraphDock.\"\"\"\n",
        "    lig_coords, lig_feats, lig_types = get_ligand_atoms_features(ligand_path)\n",
        "    if lig_coords is None:\n",
        "        return None\n",
        "\n",
        "    prot_coords, prot_feats, res_types = get_protein_atoms_features(protein_path)\n",
        "\n",
        "    data = HeteroData()\n",
        "    data['ligand'].x = torch.tensor(lig_feats, dtype=torch.float)\n",
        "    data['ligand'].pos = torch.tensor(lig_coords, dtype=torch.float)\n",
        "    data['ligand'].num_nodes = len(lig_coords)\n",
        "    data['protein'].x = torch.tensor(prot_feats, dtype=torch.float)\n",
        "    data['protein'].pos = torch.tensor(prot_coords, dtype=torch.float)\n",
        "    data['protein'].num_nodes = len(prot_coords)\n",
        "\n",
        "    # Ligand-ligand edges\n",
        "    lig_dist_matrix = distance_matrix(lig_coords, lig_coords)\n",
        "    lig_edges = []\n",
        "    lig_edge_feats = []\n",
        "    for i in range(len(lig_coords)):\n",
        "        for j in range(i + 1, len(lig_coords)):\n",
        "            if lig_dist_matrix[i, j] <= ligand_cutoff:\n",
        "                dist = lig_dist_matrix[i, j]\n",
        "                edge_feat = compute_edge_features_graphdock(lig_coords[i], lig_coords[j], dist)\n",
        "                lig_edges.append([i, j])\n",
        "                lig_edges.append([j, i])\n",
        "                lig_edge_feats.append(edge_feat)\n",
        "                lig_edge_feats.append(edge_feat)\n",
        "\n",
        "    if len(lig_edges) > 0:\n",
        "        data['ligand', 'lig_lig', 'ligand'].edge_index = torch.tensor(\n",
        "            lig_edges, dtype=torch.long).t().contiguous()\n",
        "        data['ligand', 'lig_lig', 'ligand'].edge_attr = torch.tensor(\n",
        "            lig_edge_feats, dtype=torch.float)\n",
        "\n",
        "    # Protein-protein edges\n",
        "    prot_dist_matrix = distance_matrix(prot_coords, prot_coords)\n",
        "    prot_edges = []\n",
        "    prot_edge_feats = []\n",
        "    for i in range(len(prot_coords)):\n",
        "        nearby = np.where(prot_dist_matrix[i] <= protein_cutoff)[0]\n",
        "        for j in nearby:\n",
        "            if i < j:\n",
        "                dist = prot_dist_matrix[i, j]\n",
        "                edge_feat = compute_edge_features_graphdock(prot_coords[i], prot_coords[j], dist)\n",
        "                prot_edges.append([i, j])\n",
        "                prot_edges.append([j, i])\n",
        "                prot_edge_feats.append(edge_feat)\n",
        "                prot_edge_feats.append(edge_feat)\n",
        "\n",
        "    if len(prot_edges) > 0:\n",
        "        data['protein', 'prot_prot', 'protein'].edge_index = torch.tensor(\n",
        "            prot_edges, dtype=torch.long).t().contiguous()\n",
        "        data['protein', 'prot_prot', 'protein'].edge_attr = torch.tensor(\n",
        "            prot_edge_feats, dtype=torch.float)\n",
        "\n",
        "    # Protein-ligand interaction edges\n",
        "    cross_dist_matrix = distance_matrix(lig_coords, prot_coords)\n",
        "    pl_edges = []\n",
        "    pl_edge_feats = []\n",
        "    lp_edges = []\n",
        "    lp_edge_feats = []\n",
        "\n",
        "    for i in range(len(lig_coords)):\n",
        "        for j in range(len(prot_coords)):\n",
        "            if cross_dist_matrix[i, j] <= interaction_cutoff:\n",
        "                dist = cross_dist_matrix[i, j]\n",
        "                edge_feat = compute_edge_features_graphdock(lig_coords[i], prot_coords[j], dist)\n",
        "                pl_edges.append([i, j])\n",
        "                pl_edge_feats.append(edge_feat)\n",
        "                edge_feat_rev = compute_edge_features_graphdock(prot_coords[j], lig_coords[i], dist)\n",
        "                lp_edges.append([j, i])\n",
        "                lp_edge_feats.append(edge_feat_rev)\n",
        "\n",
        "    if len(pl_edges) > 0:\n",
        "        data['ligand', 'interaction', 'protein'].edge_index = torch.tensor(\n",
        "            pl_edges, dtype=torch.long).t().contiguous()\n",
        "        data['ligand', 'interaction', 'protein'].edge_attr = torch.tensor(\n",
        "            pl_edge_feats, dtype=torch.float)\n",
        "        data['protein', 'interaction', 'ligand'].edge_index = torch.tensor(\n",
        "            lp_edges, dtype=torch.long).t().contiguous()\n",
        "        data['protein', 'interaction', 'ligand'].edge_attr = torch.tensor(\n",
        "            lp_edge_feats, dtype=torch.float)\n",
        "\n",
        "    return data\n",
        "def process_pdbbind_with_graphdock(dc_dataset, ligand_cutoff=5.0,\n",
        "                                    protein_cutoff=6.0, interaction_cutoff=5.0):\n",
        "    \"\"\"Process PDBBind dataset with GraphDock.\"\"\"\n",
        "    graph_data = []\n",
        "    failed_count = 0\n",
        "\n",
        "    for X, y, w, ids in tqdm(dc_dataset.iterbatches(batch_size=1, deterministic=True),\n",
        "                             desc=\"Processing with GraphDock\"):\n",
        "        ligand_path = X[0][0]\n",
        "        protein_path = X[0][1]\n",
        "\n",
        "        try:\n",
        "            hetero_data = create_graphdock_hetero_graph(\n",
        "                ligand_path, protein_path, ligand_cutoff, protein_cutoff, interaction_cutoff\n",
        "            )\n",
        "            if hetero_data is None:\n",
        "                failed_count += 1\n",
        "                continue\n",
        "            hetero_data.y = torch.tensor(y, dtype=torch.float)\n",
        "            graph_data.append(hetero_data)\n",
        "        except Exception as e:\n",
        "            failed_count += 1\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nProcessed {len(graph_data)} complexes successfully, Failed: {failed_count}\")\n",
        "    return graph_data"
      ],
      "metadata": {
        "id": "zmphSVVk_4CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GraphDock Model"
      ],
      "metadata": {
        "id": "a_4vQ2jAAJ8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import HeteroConv, GINConv, global_mean_pool, global_max_pool\n",
        "from torch.nn import Linear, BatchNorm1d, Sequential, ReLU, Dropout\n",
        "\n",
        "class GraphDockModel(nn.Module):\n",
        "    \"\"\"\n",
        "    GraphDock GNN model using GIN (Graph Isomorphism Network) for predicting\n",
        "    ligand-protein binding affinity.\n",
        "\n",
        "    Key features:\n",
        "    - GINConv for powerful graph representation learning\n",
        "    - Edge features incorporated via concatenation to node features\n",
        "    - Residual connections for better gradient flow\n",
        "    - Multi-level pooling (mean + max) for robust graph representations\n",
        "    \"\"\"\n",
        "    def __init__(self, ligand_feat_dim=18, protein_feature_dim=16, edge_feat_dim=14,\n",
        "                 hidden_dim=128, num_layers=3, output_dim=1, dropout=0.2):\n",
        "        super(GraphDockModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # --- Ligand embedding ---\n",
        "        self.ligand_embedding = nn.Sequential(\n",
        "            nn.Linear(ligand_feat_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # --- Protein embedding ---\n",
        "        self.protein_embedding = nn.Sequential(\n",
        "            nn.Linear(protein_feature_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # --- Edge feature encoders ---\n",
        "        self.edge_encoders = nn.ModuleDict({\n",
        "            'lig_lig': Linear(edge_feat_dim, hidden_dim),\n",
        "            'prot_prot': Linear(edge_feat_dim, hidden_dim),\n",
        "            'interaction': Linear(edge_feat_dim, hidden_dim),\n",
        "        })\n",
        "\n",
        "        # --- Heterogeneous GNN layers with GIN ---\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleDict()\n",
        "\n",
        "        for layer in range(num_layers):\n",
        "            conv_dict = {}\n",
        "\n",
        "            # GIN uses MLPs to aggregate neighbor features\n",
        "            # Each edge type gets its own GIN layer with a 2-layer MLP\n",
        "            conv_dict[('ligand', 'lig_lig', 'ligand')] = GINConv(\n",
        "                Sequential(\n",
        "                    Linear(hidden_dim * 2, hidden_dim * 2),  # *2 for node + edge features\n",
        "                    BatchNorm1d(hidden_dim * 2),\n",
        "                    ReLU(),\n",
        "                    Dropout(dropout),\n",
        "                    Linear(hidden_dim * 2, hidden_dim)\n",
        "                ),\n",
        "                train_eps=True\n",
        "            )\n",
        "\n",
        "            conv_dict[('protein', 'prot_prot', 'protein')] = GINConv(\n",
        "                Sequential(\n",
        "                    Linear(hidden_dim * 2, hidden_dim * 2),\n",
        "                    BatchNorm1d(hidden_dim * 2),\n",
        "                    ReLU(),\n",
        "                    Dropout(dropout),\n",
        "                    Linear(hidden_dim * 2, hidden_dim)\n",
        "                ),\n",
        "                train_eps=True\n",
        "            )\n",
        "\n",
        "            conv_dict[('ligand', 'interaction', 'protein')] = GINConv(\n",
        "                Sequential(\n",
        "                    Linear(hidden_dim * 2, hidden_dim * 2),\n",
        "                    BatchNorm1d(hidden_dim * 2),\n",
        "                    ReLU(),\n",
        "                    Dropout(dropout),\n",
        "                    Linear(hidden_dim * 2, hidden_dim)\n",
        "                ),\n",
        "                train_eps=True\n",
        "            )\n",
        "\n",
        "            conv_dict[('protein', 'interaction', 'ligand')] = GINConv(\n",
        "                Sequential(\n",
        "                    Linear(hidden_dim * 2, hidden_dim * 2),\n",
        "                    BatchNorm1d(hidden_dim * 2),\n",
        "                    ReLU(),\n",
        "                    Dropout(dropout),\n",
        "                    Linear(hidden_dim * 2, hidden_dim)\n",
        "                ),\n",
        "                train_eps=True\n",
        "            )\n",
        "\n",
        "            self.convs.append(HeteroConv(conv_dict, aggr='sum'))\n",
        "\n",
        "            self.batch_norms[f'ligand_{layer}'] = BatchNorm1d(hidden_dim)\n",
        "            self.batch_norms[f'protein_{layer}'] = BatchNorm1d(hidden_dim)\n",
        "\n",
        "        # --- Prediction head ---\n",
        "        # Input: 4 * hidden_dim (ligand_mean + ligand_max + protein_mean + protein_max)\n",
        "        self.predictor = Sequential(\n",
        "            Linear(hidden_dim * 4, hidden_dim * 2),\n",
        "            BatchNorm1d(hidden_dim * 2),\n",
        "            ReLU(),\n",
        "            Dropout(dropout),\n",
        "            Linear(hidden_dim * 2, hidden_dim),\n",
        "            BatchNorm1d(hidden_dim),\n",
        "            ReLU(),\n",
        "            Dropout(dropout),\n",
        "            Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        # --- Embed nodes ---\n",
        "        x_dict = {\n",
        "            'ligand': self.ligand_embedding(data['ligand'].x),\n",
        "            'protein': self.protein_embedding(data['protein'].x)\n",
        "        }\n",
        "\n",
        "        # --- Encode edge features ---\n",
        "        edge_attr_dict = {}\n",
        "        for edge_type in data.edge_types:\n",
        "            if hasattr(data[edge_type], 'edge_attr'):\n",
        "                if edge_type[1] == 'lig_lig':\n",
        "                    encoder_key = 'lig_lig'\n",
        "                elif edge_type[1] == 'prot_prot':\n",
        "                    encoder_key = 'prot_prot'\n",
        "                else:\n",
        "                    encoder_key = 'interaction'\n",
        "                edge_attr_dict[edge_type] = self.edge_encoders[encoder_key](\n",
        "                    data[edge_type].edge_attr\n",
        "                )\n",
        "\n",
        "        # --- Message passing through GNN layers with residual connections ---\n",
        "        for layer_idx, conv in enumerate(self.convs):\n",
        "            # Prepare node features by concatenating with aggregated edge features\n",
        "            # For GIN, we'll concatenate edge features with node features\n",
        "            x_dict_with_edges = {}\n",
        "            for node_type in ['ligand', 'protein']:\n",
        "                # Collect all incoming edges for this node type\n",
        "                edge_features_list = []\n",
        "                for edge_type in data.edge_types:\n",
        "                    if edge_type[2] == node_type and edge_type in edge_attr_dict:\n",
        "                        edge_index = data[edge_type].edge_index\n",
        "                        edge_attr = edge_attr_dict[edge_type]\n",
        "\n",
        "                        # Aggregate edge features to target nodes (simple mean)\n",
        "                        target_nodes = edge_index[1]\n",
        "                        num_nodes = x_dict[node_type].size(0)\n",
        "\n",
        "                        # Sum edge features for each node\n",
        "                        edge_sum = torch.zeros(num_nodes, self.hidden_dim,\n",
        "                                             device=x_dict[node_type].device)\n",
        "                        edge_sum.index_add_(0, target_nodes, edge_attr)\n",
        "\n",
        "                        # Count edges per node for averaging\n",
        "                        edge_count = torch.zeros(num_nodes, 1,\n",
        "                                               device=x_dict[node_type].device)\n",
        "                        edge_count.index_add_(0, target_nodes,\n",
        "                                            torch.ones(target_nodes.size(0), 1,\n",
        "                                                     device=x_dict[node_type].device))\n",
        "                        edge_count = edge_count.clamp(min=1)  # Avoid division by zero\n",
        "\n",
        "                        edge_features_list.append(edge_sum / edge_count)\n",
        "\n",
        "                # Average all edge features\n",
        "                if edge_features_list:\n",
        "                    avg_edge_features = torch.stack(edge_features_list).mean(dim=0)\n",
        "                else:\n",
        "                    avg_edge_features = torch.zeros_like(x_dict[node_type])\n",
        "\n",
        "                # Concatenate node features with edge features\n",
        "                x_dict_with_edges[node_type] = torch.cat(\n",
        "                    [x_dict[node_type], avg_edge_features], dim=1\n",
        "                )\n",
        "\n",
        "            # Apply GIN convolution\n",
        "            x_dict_new = conv(x_dict_with_edges, data.edge_index_dict)\n",
        "\n",
        "            # Apply batch norm and activation\n",
        "            x_dict_new = {\n",
        "                'ligand': self.batch_norms[f'ligand_{layer_idx}'](x_dict_new['ligand']),\n",
        "                'protein': self.batch_norms[f'protein_{layer_idx}'](x_dict_new['protein'])\n",
        "            }\n",
        "\n",
        "            # Residual connection: add previous layer's output\n",
        "            x_dict = {\n",
        "                'ligand': F.relu(x_dict_new['ligand'] + x_dict['ligand']),\n",
        "                'protein': F.relu(x_dict_new['protein'] + x_dict['protein'])\n",
        "            }\n",
        "\n",
        "        # --- Get batch indices ---\n",
        "        ligand_batch = data['ligand'].batch if hasattr(data['ligand'], 'batch') else torch.zeros(\n",
        "            data['ligand'].num_nodes, dtype=torch.long, device=x_dict['ligand'].device\n",
        "        )\n",
        "        protein_batch = data['protein'].batch if hasattr(data['protein'], 'batch') else torch.zeros(\n",
        "            data['protein'].num_nodes, dtype=torch.long, device=x_dict['protein'].device\n",
        "        )\n",
        "\n",
        "        # --- Multi-level pooling: combine mean and max ---\n",
        "        # Mean pooling captures overall graph structure\n",
        "        ligand_mean = global_mean_pool(x_dict['ligand'], ligand_batch)\n",
        "        protein_mean = global_mean_pool(x_dict['protein'], protein_batch)\n",
        "\n",
        "        # Max pooling captures most salient features\n",
        "        ligand_max = global_max_pool(x_dict['ligand'], ligand_batch)\n",
        "        protein_max = global_max_pool(x_dict['protein'], protein_batch)\n",
        "\n",
        "        # Concatenate all pooled representations\n",
        "        combined = torch.cat([ligand_mean, ligand_max, protein_mean, protein_max], dim=1)\n",
        "\n",
        "        return self.predictor(combined)"
      ],
      "metadata": {
        "id": "llB7cXSRB9j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Core PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "# PyTorch Geometric\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
        "import os\n",
        "\n",
        "# Enable better CUDA error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "def train_graphdock_model(model, train_loader, val_loader, num_epochs=200,\n",
        "                          patience=25, lr=0.001, device='cpu',\n",
        "                          save_path='best_model.pth'):\n",
        "    \"\"\"\n",
        "    Training loop for GraphDock model using mixed precision.\n",
        "    \"\"\"\n",
        "    print(f\"Using device: {device}\")\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=10\n",
        "    )\n",
        "\n",
        "    scaler = GradScaler('cuda' if device == 'cuda' else 'cpu')\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Starting epoch {epoch+1}\")\n",
        "\n",
        "        # --- Training step ---\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            try:\n",
        "                batch = batch.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with autocast(device_type='cuda' if device == 'cuda' else 'cpu'):\n",
        "                    preds = model(batch).squeeze()\n",
        "                    targets = batch.y.squeeze().float().to(device)\n",
        "\n",
        "                    # Check for NaN/Inf\n",
        "                    if torch.isnan(preds).any() or torch.isinf(preds).any():\n",
        "                        print(f\"Warning: NaN/Inf in predictions at batch {batch_idx}\")\n",
        "                        continue\n",
        "\n",
        "                    loss = F.mse_loss(preds, targets)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                train_losses.append(loss.item())\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "                raise e\n",
        "\n",
        "        if not train_losses:\n",
        "            print(\"Warning: No valid training batches in this epoch\")\n",
        "            continue\n",
        "\n",
        "        # --- Validation step ---\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                try:\n",
        "                    batch = batch.to(device)\n",
        "\n",
        "                    with autocast(device_type='cuda' if device == 'cuda' else 'cpu'):\n",
        "                        preds = model(batch).squeeze()\n",
        "                        targets = batch.y.squeeze().float().to(device)\n",
        "\n",
        "                        if torch.isnan(preds).any() or torch.isinf(preds).any():\n",
        "                            print(f\"Warning: NaN/Inf in validation predictions at batch {batch_idx}\")\n",
        "                            continue\n",
        "\n",
        "                        loss = F.mse_loss(preds, targets)\n",
        "                        val_losses.append(loss.item())\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in validation batch {batch_idx}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        if not val_losses:\n",
        "            print(\"Warning: No valid validation batches in this epoch\")\n",
        "            continue\n",
        "\n",
        "        # Compute average losses\n",
        "        train_loss_avg = sum(train_losses) / len(train_losses)\n",
        "        val_loss_avg = sum(val_losses) / len(val_losses)\n",
        "\n",
        "        # Adjust learning rate\n",
        "        old_lr = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step(val_loss_avg)\n",
        "        new_lr = optimizer.param_groups[0]['lr']\n",
        "        if new_lr != old_lr:\n",
        "            print(f\"Learning rate reduced to {new_lr}\")\n",
        "\n",
        "        # --- Early stopping logic ---\n",
        "        if val_loss_avg < best_val_loss:\n",
        "            best_val_loss = val_loss_avg\n",
        "            patience_counter = 0\n",
        "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': best_model_state,\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': best_val_loss,\n",
        "            }, save_path)\n",
        "\n",
        "            print(f\"  ✓ New best model saved (val_loss: {best_val_loss:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "                model.load_state_dict(best_model_state)\n",
        "                break\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss_avg:.4f} - Val Loss: {val_loss_avg:.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_dZqSHaeCYLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Function"
      ],
      "metadata": {
        "id": "Mg6wxJabBG8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, loader, device='cpu'):\n",
        "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            preds = model(batch).squeeze().cpu().numpy()\n",
        "            targets = batch.y.squeeze().cpu().numpy()\n",
        "\n",
        "            if preds.ndim == 0:\n",
        "                preds = np.array([preds])\n",
        "            if targets.ndim == 0:\n",
        "                targets = np.array([targets])\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(targets)\n",
        "\n",
        "    actuals = np.array(actuals)\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(actuals, predictions)\n",
        "\n",
        "    return mse, rmse, r2, actuals, predictions"
      ],
      "metadata": {
        "id": "voH03fftBJLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset ###"
      ],
      "metadata": {
        "id": "76UkFAqwCdGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PDBBind dataset\n",
        "tasks, datasets, transformers = dc.molnet.load_pdbbind(\n",
        "    featurizer='raw',\n",
        "    set_name='refined',\n",
        "    splitter='random',\n",
        "    reload=True\n",
        ")\n",
        "train_dataset, valid_dataset, test_dataset = datasets\n",
        "print(f\"Train dataset length: {len(train_dataset)}\\nValidate dataset length: {len(valid_dataset)}\\nTest dataset length: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJG9zTCXci51",
        "outputId": "61a191b7-a747-48c1-c0f3-ed4ff1fc8101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset length: 3881\n",
            "Validate dataset length: 485\n",
            "Test dataset length: 486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y, w, ids in train_dataset.iterbatches(batch_size=1, deterministic=True):\n",
        "    print(\"Features (X):\", X)\n",
        "    print(\"Protein pocket:\",X[0][1])\n",
        "    print(\"Label (y):\", y)\n",
        "    print(\"Weight (w):\", w)\n",
        "    print(\"ID:\", ids)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3dH7ePacnsX",
        "outputId": "d1493385-629d-4452-98b4-68bb71a2772e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features (X): [['/tmp/refined-set/4zeb/4zeb_ligand.sdf'\n",
            "  '/tmp/refined-set/4zeb/4zeb_pocket.pdb']]\n",
            "Protein pocket: /tmp/refined-set/4zeb/4zeb_pocket.pdb\n",
            "Label (y): [0.06175972]\n",
            "Weight (w): [1.]\n",
            "ID: ['4zeb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Batch\n",
        "\n",
        "class GraphDataset(Dataset):\n",
        "    from torch.serialization import add_safe_globals\n",
        "    from torch_geometric.data import Data, HeteroData\n",
        "    from torch_geometric.data.storage import BaseStorage, NodeStorage, EdgeStorage\n",
        "    add_safe_globals([BaseStorage, NodeStorage, EdgeStorage, Data, HeteroData])\n",
        "\n",
        "    def __init__(self, graph_dir, dataset_name='train'):\n",
        "        self.graph_dir = graph_dir\n",
        "        metadata_path = os.path.join(graph_dir, f\"{dataset_name}_metadata.pkl\")\n",
        "        with open(metadata_path, 'rb') as f:\n",
        "            metadata = pickle.load(f)\n",
        "        self.complex_ids = metadata['successful_ids']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.complex_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        complex_id = self.complex_ids[idx]\n",
        "        graph_path = os.path.join(self.graph_dir, f\"{complex_id}.pt\")\n",
        "        return torch.load(graph_path)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return Batch.from_data_list(batch)\n"
      ],
      "metadata": {
        "id": "kRnJbrx0BPjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_graphs = process_pdbbind_with_graphdock(\n",
        "    train_dataset,\n",
        "    ligand_cutoff=5.0,\n",
        "    protein_cutoff=6.0,\n",
        "    interaction_cutoff=5.0\n",
        ")\n",
        "\n",
        "print(\"Processing validation set...\")\n",
        "valid_graphs = process_pdbbind_with_graphdock(\n",
        "    valid_dataset,\n",
        "    ligand_cutoff=5.0,\n",
        "    protein_cutoff=6.0,\n",
        "    interaction_cutoff=5.0\n",
        ")\n",
        "\n",
        "print(\"Processing test set...\")\n",
        "test_graphs = process_pdbbind_with_graphdock(\n",
        "    test_dataset,\n",
        "    ligand_cutoff=5.0,\n",
        "    protein_cutoff=6.0,\n",
        "    interaction_cutoff=5.0\n",
        ")\n",
        "\n",
        "# May not be actual size because of invalid molecule structure\n",
        "print(f\"Train graphs have size of {len(train_graphs)}\")\n",
        "print(f\"Valid graphs have size of {len(valid_graphs)}\")\n",
        "print(f\"Test graphs have size of {len(test_graphs)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JsPV_UyGTsAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418763f6-bf61-499d-d6e9-7a4dce3d73e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing with GraphDock: 3881it [12:00,  5.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed 2212 complexes successfully, Failed: 1669\n",
            "Processing validation set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing with GraphDock: 485it [01:38,  4.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed 284 complexes successfully, Failed: 201\n",
            "Processing test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing with GraphDock: 486it [01:36,  5.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed 291 complexes successfully, Failed: 195\n",
            "Train graphs have size of 2212\n",
            "Valid graphs have size of 284\n",
            "Test graphs have size of 291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_graphs, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_graphs, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "UUwG4i-Plg-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch\n",
        "data_iter = iter(train_loader)\n",
        "batch = next(data_iter)\n",
        "\n",
        "# Inspect batch type\n",
        "print(\"Batch type:\", type(batch))\n",
        "\n",
        "# Inspect ligand nodes\n",
        "print(\"Ligand node features shape:\", batch['ligand'].x.shape)\n",
        "print(\"First 5 ligand node features:\\n\", batch['ligand'].x[:5])\n",
        "\n",
        "# Inspect protein nodes\n",
        "print(\"Protein node features shape:\", batch['protein'].x.shape)\n",
        "print(\"First 5 protein node features:\\n\", batch['protein'].x[:5])\n",
        "\n",
        "# Inspect edges\n",
        "print(\"Edge types:\", batch.edge_index_dict.keys())\n",
        "for etype, edge_index in batch.edge_index_dict.items():\n",
        "    print(f\"{etype} edge_index shape:\", edge_index.shape)\n",
        "    if batch[etype].edge_attr is not None:\n",
        "        print(f\"{etype} edge_attr shape:\", batch[etype].edge_attr.shape)\n",
        "\n",
        "# Optional: look at batch indices\n",
        "if hasattr(batch['ligand'], 'batch'):\n",
        "    print(\"Ligand batch indices:\", batch['ligand'].batch[:10])\n",
        "if hasattr(batch['protein'], 'batch'):\n",
        "    print(\"Protein batch indices:\", batch['protein'].batch[:10])\n"
      ],
      "metadata": {
        "id": "fbVTzVzvmfMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c795f09-832e-4606-e29d-990114c0521b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch type: <class 'abc.HeteroDataBatch'>\n",
            "Ligand node features shape: torch.Size([809, 18])\n",
            "First 5 ligand node features:\n",
            " tensor([[0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.3333, 0.0000, 0.6667, 0.0000, 0.0000, 0.1600, 0.0000],\n",
            "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.6667, 0.0000, 0.6667, 0.0000, 0.0000, 0.1201, 1.0000],\n",
            "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.6667, 0.0000, 0.6667, 0.0000, 0.0000, 0.1201, 1.0000],\n",
            "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.6667, 1.0000, 0.6667, 0.0000, 0.0000, 0.1401, 1.0000],\n",
            "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.6667, 0.0000, 0.6667, 0.0000, 0.0000, 0.1201, 1.0000]])\n",
            "Protein node features shape: torch.Size([7703, 16])\n",
            "First 5 protein node features:\n",
            " tensor([[0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.3000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.5000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.3000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.5000],\n",
            "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.3000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.5000],\n",
            "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.3000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.5000],\n",
            "        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.3000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.5000]])\n",
            "Edge types: dict_keys([('ligand', 'lig_lig', 'ligand'), ('protein', 'prot_prot', 'protein'), ('ligand', 'interaction', 'protein'), ('protein', 'interaction', 'ligand')])\n",
            "('ligand', 'lig_lig', 'ligand') edge_index shape: torch.Size([2, 17448])\n",
            "('ligand', 'lig_lig', 'ligand') edge_attr shape: torch.Size([17448, 14])\n",
            "('protein', 'prot_prot', 'protein') edge_index shape: torch.Size([2, 289250])\n",
            "('protein', 'prot_prot', 'protein') edge_attr shape: torch.Size([289250, 14])\n",
            "('ligand', 'interaction', 'protein') edge_index shape: torch.Size([2, 11042])\n",
            "('ligand', 'interaction', 'protein') edge_attr shape: torch.Size([11042, 14])\n",
            "('protein', 'interaction', 'ligand') edge_index shape: torch.Size([2, 11042])\n",
            "('protein', 'interaction', 'ligand') edge_attr shape: torch.Size([11042, 14])\n",
            "Ligand batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Protein batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using {device}.\")\n",
        "\n",
        "# --- Model ---\n",
        "model = GraphDockModel(\n",
        "    ligand_feat_dim=18,\n",
        "    protein_feature_dim=16,\n",
        "    edge_feat_dim=14,\n",
        "    hidden_dim=128,\n",
        "    num_layers=3,\n",
        "    output_dim=1,\n",
        "    dropout=0.2\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "6ikZoFa4ImxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683650df-5c57-4d62-e974-8699bcbb531f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training ---\n",
        "trained_model = train_graphdock_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=valid_loader,\n",
        "    num_epochs=50,  # fewer epochs for testing\n",
        "    patience=10,\n",
        "    lr=0.001,\n",
        "    device=device,\n",
        "    save_path='graphdock_bestmodel.pth'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gNwfIbeGxlf",
        "outputId": "84d35bd6-bb04-4dac-9abd-739dc3cc27e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ New best model saved (val_loss: 0.6497)\n",
            "Epoch 1/50 - Train Loss: 0.8057 - Val Loss: 0.6497\n",
            "Starting epoch 2\n",
            "Epoch 2/50 - Train Loss: 0.6609 - Val Loss: 0.7550\n",
            "Starting epoch 3\n",
            "Epoch 3/50 - Train Loss: 0.6193 - Val Loss: 0.8508\n",
            "Starting epoch 4\n",
            "  ✓ New best model saved (val_loss: 0.6231)\n",
            "Epoch 4/50 - Train Loss: 0.5768 - Val Loss: 0.6231\n",
            "Starting epoch 5\n",
            "  ✓ New best model saved (val_loss: 0.5949)\n",
            "Epoch 5/50 - Train Loss: 0.5488 - Val Loss: 0.5949\n",
            "Starting epoch 6\n",
            "  ✓ New best model saved (val_loss: 0.5654)\n",
            "Epoch 6/50 - Train Loss: 0.5084 - Val Loss: 0.5654\n",
            "Starting epoch 7\n",
            "Epoch 7/50 - Train Loss: 0.4875 - Val Loss: 0.7560\n",
            "Starting epoch 8\n",
            "  ✓ New best model saved (val_loss: 0.5474)\n",
            "Epoch 8/50 - Train Loss: 0.4663 - Val Loss: 0.5474\n",
            "Starting epoch 9\n",
            "  ✓ New best model saved (val_loss: 0.5363)\n",
            "Epoch 9/50 - Train Loss: 0.4610 - Val Loss: 0.5363\n",
            "Starting epoch 10\n",
            "Epoch 10/50 - Train Loss: 0.4312 - Val Loss: 0.7729\n",
            "Starting epoch 11\n",
            "Epoch 11/50 - Train Loss: 0.4395 - Val Loss: 0.5697\n",
            "Starting epoch 12\n",
            "Epoch 12/50 - Train Loss: 0.4212 - Val Loss: 0.5828\n",
            "Starting epoch 13\n",
            "Epoch 13/50 - Train Loss: 0.3920 - Val Loss: 0.6241\n",
            "Starting epoch 14\n",
            "Epoch 14/50 - Train Loss: 0.3870 - Val Loss: 0.6405\n",
            "Starting epoch 15\n",
            "  ✓ New best model saved (val_loss: 0.5233)\n",
            "Epoch 15/50 - Train Loss: 0.3812 - Val Loss: 0.5233\n",
            "Starting epoch 16\n",
            "Epoch 16/50 - Train Loss: 0.3555 - Val Loss: 0.6357\n",
            "Starting epoch 17\n",
            "Epoch 17/50 - Train Loss: 0.3390 - Val Loss: 0.6088\n",
            "Starting epoch 18\n",
            "Epoch 18/50 - Train Loss: 0.3147 - Val Loss: 0.5726\n",
            "Starting epoch 19\n",
            "Epoch 19/50 - Train Loss: 0.3165 - Val Loss: 0.5549\n",
            "Starting epoch 20\n",
            "Epoch 20/50 - Train Loss: 0.3127 - Val Loss: 0.6238\n",
            "Starting epoch 21\n",
            "Epoch 21/50 - Train Loss: 0.3051 - Val Loss: 0.8758\n",
            "Starting epoch 22\n",
            "Epoch 22/50 - Train Loss: 0.2916 - Val Loss: 0.7968\n",
            "Starting epoch 23\n",
            "Epoch 23/50 - Train Loss: 0.2798 - Val Loss: 0.5663\n",
            "Starting epoch 24\n",
            "Epoch 24/50 - Train Loss: 0.2968 - Val Loss: 0.5854\n",
            "Starting epoch 25\n",
            "\n",
            "Early stopping at epoch 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation"
      ],
      "metadata": {
        "id": "dtqwgTObFfyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on validation set\n",
        "val_mse, val_rmse, val_r2, val_actuals, val_preds = evaluate_model(\n",
        "    trained_model, valid_loader, device=device\n",
        ")\n",
        "\n",
        "print(f\"Validation MSE: {val_mse:.4f}\")\n",
        "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
        "print(f\"Validation R²: {val_r2:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_mse, test_rmse, test_r2, test_actuals, test_preds = evaluate_model(\n",
        "    trained_model, test_loader, device=device\n",
        ")\n",
        "\n",
        "print(f\"Test MSE: {test_mse:.4f}\")\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "print(f\"Test R²: {test_r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "wQ797VreFhi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f719430e-4fd1-4afd-f250-3af3554bef3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 0.5140\n",
            "Validation RMSE: 0.7169\n",
            "Validation R²: 0.4709\n",
            "Test MSE: 0.4252\n",
            "Test RMSE: 0.6521\n",
            "Test R²: 0.5744\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}